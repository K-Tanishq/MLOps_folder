{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "id": "z3ErL8ryAJNa"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "import torchvision\n",
        "from torchvision import transforms, datasets\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "import torch.nn as nn\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "import torch.quantization\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "import time\n",
        "import sys"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "transform = transforms.Compose([transforms.ToTensor()])\n",
        "train_dataset = torchvision.datasets.MNIST(root=\"~/torch_datasets\", train=True, transform=transform, download=True)\n",
        "test_dataset = torchvision.datasets.MNIST(root=\"~/torch_datasets\", train=False, transform=transform, download=True)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)"
      ],
      "metadata": {
        "id": "QdiclPYiAbhn"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class LogisticRegressionModel(nn.Module):\n",
        "    def __init__(self, input_dim, output_dim):\n",
        "        super().__init__()\n",
        "        self.linear = nn.Linear(input_dim, output_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.view(x.size(0), -1)  # Flatten the input image\n",
        "        out = self.linear(x)\n",
        "        return out\n",
        "\n",
        "# Set input/output dimensions for MNIST\n",
        "input_dim = 28 * 28  # 28x28 pixels\n",
        "output_dim = 10  # 10 classes\n",
        "\n",
        "# Initialize the logistic regression model\n",
        "model = LogisticRegressionModel(input_dim, output_dim)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr= 0.01)\n"
      ],
      "metadata": {
        "id": "pd6BOuuWBWMz"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Training loop\n",
        "num_epochs = 5\n",
        "model.train()\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    for images, labels in train_loader:\n",
        "        # Forward pass\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "\n",
        "        # Backward pass and optimization\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HX8WfSrdMDJa",
        "outputId": "c5f13e11-0a95-4935-b4b0-3b0fb5b2671c"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/5], Loss: 0.3717\n",
            "Epoch [2/5], Loss: 0.4222\n",
            "Epoch [3/5], Loss: 0.1103\n",
            "Epoch [4/5], Loss: 0.3555\n",
            "Epoch [5/5], Loss: 0.1049\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.eval()\n",
        "all_preds, all_labels = [], []\n",
        "with torch.no_grad():\n",
        "    for images, labels in test_loader:\n",
        "        outputs = model(images)\n",
        "        _, predicted = torch.max(outputs, 1)\n",
        "        all_preds.extend(predicted.numpy())\n",
        "        all_labels.extend(labels.numpy())\n",
        "\n",
        "# Report accuracy\n",
        "accuracy = accuracy_score(all_labels, all_preds)\n",
        "print(f\"Original Model Accuracy: {accuracy:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LMIhvui6QCyw",
        "outputId": "1b0635e8-edce-4a3b-f9d9-29faefb6eca9"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original Model Accuracy: 0.9109\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Get model size in bytes\n",
        "def get_model_size(model):\n",
        "    total_size = 0\n",
        "    for param in model.parameters():\n",
        "        total_size += param.nelement() * param.element_size()\n",
        "    return total_size\n",
        "\n",
        "model_size = get_model_size(model)\n",
        "print(f\"Original Model Size: {model_size} bytes\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nBlg8AC_QVSt",
        "outputId": "91a1efe7-b8ab-4553-ff15-d365403250f9"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original Model Size: 31400 bytes\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Measure inference time\n",
        "start_time = time.time()\n",
        "with torch.no_grad():\n",
        "    for images, _ in test_loader:\n",
        "        outputs = model(images)\n",
        "inference_time = time.time() - start_time\n",
        "print(f\"Inference Time: {inference_time:.6f} seconds\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "by66xYsoQXYV",
        "outputId": "56947de3-989b-4168-abd7-361bf3cd0608"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Inference Time: 1.369716 seconds\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def model_size(model, quantized=False):\n",
        "  total_size = 0\n",
        "  total_params = 0\n",
        "  if quantized==False:\n",
        "     for layer in model.children():\n",
        "        if hasattr(layer, 'weight') and layer.weight is not None:\n",
        "          weight_tensor = layer.weight.data\n",
        "          weight_size = len(weight_tensor.flatten())\n",
        "          total_params+= weight_size*4\n",
        "\n",
        "\n",
        "        if hasattr(layer, 'bias') and layer.bias is not None:\n",
        "          bias_tensor = layer.bias.data\n",
        "          bias_size = len(bias_tensor.flatten())\n",
        "          total_params+= bias_size*4\n",
        "\n",
        "  else:\n",
        "    for layer in model.children():\n",
        "      if hasattr(layer, 'weight') and layer.weight is not None:\n",
        "        weight_tensor = layer.weight()\n",
        "        weight_size = len(weight_tensor.flatten())\n",
        "        total_params+= weight_size*1\n",
        "\n",
        "\n",
        "      if hasattr(layer, 'bias') and layer.bias is not None:\n",
        "        bias_tensor = layer.bias()\n",
        "        bias_size = len(bias_tensor.flatten())\n",
        "        total_params+= bias_size*4\n",
        "\n",
        "  return total_params, total_size"
      ],
      "metadata": {
        "id": "1X50xkBjR3LS"
      },
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# For the original model\n",
        "params, size = model_size(model, quantized=False)\n",
        "print(f\"Original Model size: {params}\")\n",
        "quantized_model = torch.quantization.quantize_dynamic(model, {nn.Linear}, dtype=torch.qint8)\n",
        "# For the quantized model\n",
        "quant_params, quant_size = model_size(quantized_model, quantized=True)\n",
        "print(f\"Quantized Model size: {quant_params}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hbj1EZ8gU-ix",
        "outputId": "0e7145b2-ca33-4075-d0f7-3a9e2c85b8c1"
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original Model size: 31400\n",
            "Quantized Model size: 7880\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Quantization (Dynamic Quantization in PyTorch)\n",
        "quantized_model = torch.quantization.quantize_dynamic(model, {nn.Linear}, dtype=torch.qint8)\n",
        "\n",
        "# Measure inference time for quantized model\n",
        "start_time = time.time()\n",
        "with torch.no_grad():\n",
        "    for images, _ in test_loader:\n",
        "        outputs = quantized_model(images)\n",
        "quantized_inference_time = time.time() - start_time\n",
        "print(f\"Quantized Inference Time: {quantized_inference_time:.6f} seconds\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v_Wjq4fIQcgd",
        "outputId": "80a1cf1e-24ba-4bae-a266-ca0c5f0c693c"
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Quantized Inference Time: 1.669748 seconds\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate quantized model\n",
        "quantized_preds = []\n",
        "with torch.no_grad():\n",
        "    for images, _ in test_loader:\n",
        "        outputs = quantized_model(images)\n",
        "        _, predicted = torch.max(outputs, 1)\n",
        "        quantized_preds.extend(predicted.numpy())\n",
        "\n",
        "quantized_accuracy = accuracy_score(all_labels, quantized_preds)\n",
        "print(f\"Quantized Model Accuracy: {quantized_accuracy:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "liHUio_mQe3l",
        "outputId": "d4ef8aef-566e-4124-b517-26d15e41fe5f"
      },
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Quantized Model Accuracy: 0.9102\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Model size comparison\n",
        "print(f\"Model Size Comparison:\\nOriginal Model: {params} bytes\\nQuantized Model: {quant_params} bytes\")\n",
        "print(f\"\\nAccuracy Comparison:\\nOriginal Model: {accuracy*100:.2f}%\\nQuantized Model: {quantized_accuracy*100:.2f}%\")\n",
        "print(f\"\\nInference Time Comparison:\\nOriginal Model: {inference_time:.6f} seconds\\nQuantized Model: {quantized_inference_time:.6f} seconds\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z9LMxTdFQRE8",
        "outputId": "f8216c24-0db5-4e57-eabc-c73595c5db7b"
      },
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model Size Comparison:\n",
            "Original Model: 31400 bytes\n",
            "Quantized Model: 7880 bytes\n",
            "\n",
            "Accuracy Comparison:\n",
            "Original Model: 91.09%\n",
            "Quantized Model: 91.02%\n",
            "\n",
            "Inference Time Comparison:\n",
            "Original Model: 1.369716 seconds\n",
            "Quantized Model: 1.669748 seconds\n"
          ]
        }
      ]
    }
  ]
}